kubernetes
 架构：master/agent
	master主机:
		*kube-apiserver
		*kube-scheduler
		*kube-controller-manager
		docker
		etcd
		cloud-controller-manager
	agent主机（node）：
		kubelet
		docker
		kube-proxy
		container runtime
	addons
		dns
		web UI(Dashboard)
		container resource monitoring
		cluster-level logging
		
Pod  一组容器
Labels 用于标识pods的标签，对相同标签的pod进行管理
Kubelet 容器代理
kube-proxy pods的负载均衡
etcd 元数据服务
cAdvisor 提供资源使用/性能统计
Replication Controller 管理PODS副本
Scheduler 在工作节点调度pods
API Server Kubernetes API server

basic Kubernetes objects include 
		Pod
		Service
		Namespace
		Volume
k8s cluster 容器编排系统
	核心任务：容器编排
	容器：应用程序
	pod controller，deployment
	
用户和pod之间通过service衔接 service可以对pod进行反代调度，可以提供负载均衡	

网络
service网络
pod网络
节点网络

---------------------------------------------------------------------------------
k8s安装部署
centos7.5
master 192.168.47.141
node1  192.168.47.142
node2  192.168.47.143

docker-ce 18.09.3
kubectl-1.13.4-0.x86_64
kubeadm-1.13.4-0.x86_64
kubectl-1.13.4-0.x86_64

一、
ssh互信
ntpd时间同步
hosts域名解析
关闭firewall和selinux
禁用swap  临时禁用命令swapoff -a  永久禁用在fstab文件中编辑

配置yum仓库
aliyun的kubernetes库和docker-ce库
cat base.repo
[base-aliyun]
name=aliyun base
baseurl=https://mirrors.aliyun.com/centos/7/os/x86_64/
gpgcheck=0
[updates]
name=aliyun updates
baseurl=https://mirrors.aliyun.com/centos/7/updates/x86_64/
gpgcheck=0
[extras]
name=aliyun extrax
baseurl=https://mirrors.aliyun.com/centos/7/extras/x86_64/
gpgcheck=0

cat epel.repo
[epel]
name=epel-aliyun
baseurl=https://mirrors.aliyun.com/epel/7/x86_64/
gpgcheck=0
enabled=1

cat docker-ce.repo
[docker-ce]
name=docker-ce
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/
gpgcheck=0
enabled=1

cat kubernetes.repo
[k8s]
name=k8s
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=0
enabled=1

二、三节点安装相关程序包
yum install docker-ce -y
yum install kubelet kubeadm kubectl -y

三、在各节点启动docker服务
若要通过默认的k8s.gcr.io镜象获取kubernetes系统组件的相关镜象，需要配置docker.service的Unit file中的Environment变量，为其定义合适的HTTPS_PROXY,格式如下
	Environment="HTTPS_PROXY=http://www.ik8s.io:10070"
	Environment="NO_PROXY=192.168.0.0/16,127.0.0.0/8"
	ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT


重载完成后启动docker服务
systemctl daemon-reload
systemctl start docker
systemctl enable docker
   	
[root@k8s-master ~]# sysctl -a | grep bridge
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

vim /etc/sysctl.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

# sysctl -p
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

四、初始化主节点master
systemctl enable kubelet
systemctl start kubelet
1、若未禁用Swap设备，则需要编辑kubelet的配置文件，设置其忽略swap启用的状态错误，内容如下：
[root@k8s-master ~]# vim /etc/sysconfig/kubelet
KUBELET_EXTRA_ARGS="--fail-swap-on=false"

可选步骤：在运行初始化命令之前先运行如下命令单独获取相关镜象文件，而后再运行后面的kubeadm init命令，以便于观察到镜象文件的下载过程
# kubeadm config images pull
然后即可进行master节点的初始化操作。

主节点镜像
[root@k8s-master ~]# kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.13.4
k8s.gcr.io/kube-controller-manager:v1.13.4
k8s.gcr.io/kube-scheduler:v1.13.4
k8s.gcr.io/kube-proxy:v1.13.4
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.2.24
k8s.gcr.io/coredns:1.2.6
[root@k8s-master ~]# kubeadm config images pull
[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.13.4
[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.13.4
[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.13.4
[config/images] Pulled k8s.gcr.io/kube-proxy:v1.13.4
[config/images] Pulled k8s.gcr.io/pause:3.1
[config/images] Pulled k8s.gcr.io/etcd:3.2.24
[config/images] Pulled k8s.gcr.io/coredns:1.2.6
[root@k8s-master ~]# docker image ls
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy                v1.13.4             fadcc5d2b066        8 days ago          80.3MB
k8s.gcr.io/kube-controller-manager   v1.13.4             40a817357014        8 days ago          146MB
k8s.gcr.io/kube-scheduler            v1.13.4             dd862b749309        8 days ago          79.6MB
k8s.gcr.io/kube-apiserver            v1.13.4             fc3801f0fc54        8 days ago          181MB
quay.io/coreos/flannel               v0.11.0-amd64       ff281650a721        5 weeks ago         52.6MB
k8s.gcr.io/coredns                   1.2.6               f59dcacceff4        4 months ago        40MB
k8s.gcr.io/etcd                      3.2.24              3cab8e1b9802        5 months ago        220MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        14 months ago       742kB


kubeadm init命令支持两种初始化方式：
	1、通过命令行选项传递关键的部署设定
	2、基于yaml格式的专用配置文件，允许用户自定义各个部署参数

2、初始化
[root@k8s-master ~]# kubeadm init --kubernetes-version="v1.13.4" --pod-network-cidr="10.244.0.0/16"	
Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

    kubeadm join 192.168.47.141:6443 --token oy6au7.jm9jxdk3zbeuzhan --discovery-token-ca-cert-hash sha256:5821ac17bbc0834300d7981e36eeaa240e20d9dbf64ac37e2ba54863f1e3aeba

[root@k8s-master ~]# mkdir .kube
[root@k8s-master ~]# cp /etc/kubernetes/admin.conf .kube/config

查看节点kubectl get node
[root@k8s-master ~]# kubectl get node
NAME         STATUS     ROLES    AGE   VERSION
k8s-master   NotReady   master   26m   v1.13.4


3、安装部署pod网络   flannel
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
[root@k8s-master ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
podsecuritypolicy.extensions/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.extensions/kube-flannel-ds-amd64 created
daemonset.extensions/kube-flannel-ds-arm64 created
daemonset.extensions/kube-flannel-ds-arm created
daemonset.extensions/kube-flannel-ds-ppc64le created
daemonset.extensions/kube-flannel-ds-s390x created

查看节点kubectl get node
[root@k8s-master ~]# kubectl get node
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   3m53s   v1.13.4


查看pods状态
[root@k8s-master ~]# kubectl get pods -n kube-system
NAME                                 READY   STATUS    RESTARTS   AGE
coredns-86c58d9df4-gmf9n             1/1     Running   0          34m
coredns-86c58d9df4-h9qbx             1/1     Running   0          34m
etcd-k8s-master                      1/1     Running   0          34m
kube-apiserver-k8s-master            1/1     Running   0          34m
kube-controller-manager-k8s-master   1/1     Running   0          34m
kube-flannel-ds-amd64-4qgps          1/1     Running   0          4m32s
kube-proxy-gh2cb                     1/1     Running   0          34m
kube-scheduler-k8s-master            1/1     Running   0          34m

[root@k8s-master ~]# kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   36m   v1.13.4


添加节点：
在各node节点运行
#   kubeadm join 192.168.47.141:6443 --token oy6au7.jm9jxdk3zbeuzhan --discovery-token-ca-cert-hash sha256:5821ac17bbc0834300d7981e36eeaa240e20d9dbf64ac37e2ba54863f1e3aeba

没有禁用swap时  kubeadm join 192.168.47.141:6443 --token oy6au7.jm9jxdk3zbeuzhan --discovery-token-ca-cert-hash sha256:5821ac17bbc0834300d7981e36eeaa240e20d9dbf64ac37e2ba54863f1e3aeba --ignore-preflight-errors=swap

输出部分信息如下：
 This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.
Run 'kubectl get nodes' on the master to see this node join the cluster. 

node节点docker镜像
# docker image ls
REPOSITORY               TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy    v1.13.4             fadcc5d2b066        8 days ago          80.3MB
quay.io/coreos/flannel   v0.11.0-amd64       ff281650a721        5 weeks ago         52.6MB
k8s.gcr.io/pause         3.1                 da86e6ba6ca1        14 months ago       742kB


此时在主节点查看node信息
[root@k8s-master ~]# kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   56m     v1.13.4
k8s-node1    Ready    <none>   7m56s   v1.13.4
k8s-node2    Ready    <none>   7m42s   v1.13.4

------------------------------------------------------------------------------------------
如何解决从k8s.gcr.io拉取镜像失败问题
解决方案
docker.io仓库对google的容器做了镜像，可以通过下列命令下拉取相关镜像：

docker pull mirrorgooglecontainers/kube-apiserver-amd64:v1.11.3
docker pull mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.3
docker pull mirrorgooglecontainers/kube-scheduler-amd64:v1.11.3
docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.11.3
docker pull mirrorgooglecontainers/pause:3.1
docker pull mirrorgooglecontainers/etcd-amd64:3.2.18
docker pull coredns/coredns:1.1.3
版本信息需要根据实际情况进行相应的修改。通过docker tag命令来修改镜像的标签：

docker tag docker.io/mirrorgooglecontainers/kube-proxy-amd64:v1.11.3 k8s.gcr.io/kube-proxy-amd64:v1.11.3
docker tag docker.io/mirrorgooglecontainers/kube-scheduler-amd64:v1.11.3 k8s.gcr.io/kube-scheduler-amd64:v1.11.3
docker tag docker.io/mirrorgooglecontainers/kube-apiserver-amd64:v1.11.3 k8s.gcr.io/kube-apiserver-amd64:v1.11.3
docker tag docker.io/mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.3 k8s.gcr.io/kube-controller-manager-amd64:v1.11.3
docker tag docker.io/mirrorgooglecontainers/etcd-amd64:3.2.18  k8s.gcr.io/etcd-amd64:3.2.18
docker tag docker.io/mirrorgooglecontainers/pause:3.1  k8s.gcr.io/pause:3.1
docker tag docker.io/coredns/coredns:1.1.3  k8s.gcr.io/coredns:1.1.3
使用docker rmi删除不用镜像，通过docker images命令显示，已经有我们需要的镜像文件，可以继续部署工作了：

[root@zookeeper01 jinguang1]# docker images
REPOSITORY                                                               TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy-amd64                                              v1.11.3             be5a6e1ecfa6        10 days ago         97.8 MB
k8s.gcr.io/kube-scheduler-amd64                                          v1.11.3             ca1f38854f74        10 days ago         56.8 MB
k8s.gcr.io/kube-apiserver-amd64                                          v1.11.3             3de571b6587b        10 days ago         187 MB
coredns/coredns                                                          1.1.3               b3b94275d97c        3 months ago        45.6 MB
k8s.gcr.io/coredns                                                       1.1.3               b3b94275d97c        3 months ago        45.6 MB
k8s.gcr.io/etcd-amd64                                                    3.2.18              b8df3b177be2        5 months ago        219 MB
k8s.gcr.io/pause                                                         3.1                 da86e6ba6ca1        9 months ago        742 kB
------------------------------------------------------------------------------------------

添加dashboard
kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
删除
kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml





列出指定命名的pod
# kubectl get pods -n kube-system

查看指定pod resource的详细信息
 kubectl describe pod [POD_NAME] -n kube-system 

删除指定pod resource的详细信息
 kubectl delete pod [POD_NAME] -n kube-system  

查看指定pod resource的日志文件
 kubectl logs [POD_NAME] -n kube-system 

kubectl get po # 查看目前所有的pod
kubectl get rs # 查看目前所有的replica set
kubectl get deployment # 查看目前所有的deployment

k8s删除资源状态一直是Terminating。此为背景。
解决方法：
   可使用kubectl中的强制删除命令
    # 删除POD
    kubectl delete pod PODNAME --force --grace-period=0
    # 删除NAMESPACE
    kubectl delete namespace NAMESPACENAME --force --grace-period=0
	
	
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml

# kubectl get svc -n kube-system
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
kube-dns               ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP   6h7m
kubernetes-dashboard   ClusterIP   10.103.109.218   <none>        443/TCP         7m26s

# kubectl patch svc kubernetes-dashboard -p '{"spec":{"type":"NodePort"}}' -n kube-system
service/kubernetes-dashboard patched

# kubectl get svc -n kube-system
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
kube-dns               ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP   6h13m
kubernetes-dashboard   NodePort    10.103.109.218   <none>        443:32646/TCP   14m

